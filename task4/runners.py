""" RL env runner """
import numpy as np
from collections import defaultdict


class EnvRunner:
    """
    Reinforcement learning runner in an environment with given policy
    """
    def __init__(self, env, policy, nsteps, transforms=None, step_var=None):
        self.env = env
        self.policy = policy
        self.nsteps = nsteps
        self.transforms = transforms or []
        self.step_var = step_var if step_var is not None else 0
        self.state = {"latest_observation": self.env.reset()[0]}

    @property
    def nenvs(self):
        """
        Returns number of batched envs or `None` if env is not batched
        """
        return getattr(self.env.unwrapped, "nenvs", None)

    def reset(self, **kwargs):
        """
        Resets env and runner states
        """
        self.state["latest_observation"] = self.env.reset(**kwargs)[0]
        self.policy.reset()

    def add_summary(self, name, val):
        """
        Writes logs
        """
        add_summary = self.env.get_wrapper_attr("add_summary")
        add_summary(name, val)

    def get_next(self):
        """
        Runs the agent in the environment
        """
        trajectory = defaultdict(list, {"actions": []})
        observations = []
        rewards = []
        resets = []
        self.state["env_steps"] = self.nsteps

        for i in range(self.nsteps):
            observations.append(self.state["latest_observation"])
            act = self.policy.act(self.state["latest_observation"])
            if "actions" not in act:
                raise ValueError(
                    "result of policy.act must contain 'actions' "
                    f"but has keys {list(act.keys())}"
                )
            for key, val in act.items():
                trajectory[key].append(val)

            obs, rew, terminated, truncated, _ = self.env.step(
                trajectory["actions"][-1]
            )
            self.state["latest_observation"] = obs
            rewards.append(rew)
            reset = np.logical_or(terminated, truncated)
            resets.append(reset)
            self.step_var += self.nenvs or 1

            # Only reset if the env is not batched. Batched envs should
            # auto-reset.
            if not self.nenvs and np.all(reset):
                self.state["env_steps"] = i + 1
                self.state["latest_observation"] = self.env.reset()[0]

        trajectory.update(observations=observations, rewards=rewards, resets=resets)
        trajectory["state"] = self.state

        for transform in self.transforms:
            transform(trajectory)
        return trajectory
