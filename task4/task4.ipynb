{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация алгоритма Advantage-Actor Critic (A2C) (вплоть до 10 баллов)\n",
    "\n",
    "#### дедлайн задания (сразу жёсткий): 26 апреля, 23:59 UTC+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа выполнена: Ахмаджонов Мумтозбек, М05-317."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной работе Вам предстоит реализовать алгоритм `Advantage Actor Critic`, обучаемый на батче из сред `Atari 2600`, работающих параллельно.\n",
    "\n",
    "Для начала будут использованы обёртки сред, реализованные в файле `atari_wrappers.py`. Эти обёртки предварительно обрабатывают наблюдения (производят преобразования размера, цвета фрейма, взятия максимума между фреймами, пропускают часть фреймов и сводят несколько фреймов в один большой) и вознаграждения. Некоторые обёртки помогают автоматически перезапустить среду и присвоить переменной `done` значение `True` в случае смерти агента. Файл `env_batch.py` включает в себя реализацию класса `ParallelEnvBatch`, позволяющего запускать несколько сред параллельно. Для создания (инициализации) среды можно воспользоваться функцией `nature_dqn_env`. Обратите внимание, что в случае использования `PyTorch` (https://pytorch.org/) без `tensorboardX` (https://github.com/lanpa/tensorboardX) потребуется самостоятельно реализовать обёртку среды, которая будет логрировать **исходные** суммарные награды, которые *исходная* среда возвращает, и переопределить реализацию функции `nature_dqn_env`. То есть настоятельно рекомендуется применить `tensorboardX`.\n",
    "\n",
    "Псевдокод алгоритма `Advantage Actor Critic (A2C)` приведён в **Разделе 5.2.5 (Алгоритм 20)** конспекта лекций: https://arxiv.org/pdf/2201.09746.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрипты в данной работе используют Python версию библиотеки [OpenCV](https://pypi.org/project/opencv-python/). Для запуска сред ATARI понадобится библиотека [The Arcade Learning Environment](https://github.com/Farama-Foundation/Arcade-Learning-Environment), а также библиотека [Shimmy](https://pypi.org/project/Shimmy/). Может потребоваться установка `ffmpeg` кодека. Для Unix-based операционной системы с пакетным менеджером APT может потребоваться следующая последовательность команд:\n",
    "```\n",
    "sudo apt-get install -y xvfb x11-utils ffmpeg python-opengl\n",
    "pip install pyglet pyvirtualdisplay opencv-python tqdm numpy moviepy gymnasium[atari]\n",
    "pip install gymnasium[accept-rom-license] #run 'pip install autorom; AutoROM --accept-license' if this fails\n",
    "```\n",
    "Для MacOS систем рекомендуется установить репозиторий [TFM](https://github.com/serrodcal-MII/TFM/tree/main) и выполнить инструкции из [README.md](https://github.com/serrodcal-MII/TFM/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import HTML\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from atari_wrappers import nature_dqn_env\n",
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ../xvfb: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# XVFB будет запущен в случае исполнения на сервере\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    ! bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "nenvs = 8\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, summaries = \"Tensorboard\")\n",
    "                                                    # nenvs -- количество параллельно запущенных сред\n",
    "                                                    # данный параметр можно варьировать для баланса\n",
    "                                                    # производительность итерации/надёжность Монте-Карло оценок\n",
    "                                                    # помните: при уменьшении nenvs, возможно, придётся\n",
    "                                                    # увеличить количество итераций оптимизации\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs, _ = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующим шагом будет реализация модели, которая выводит логиты для категориального распределения на действия и оценку на значения $V$-функции ценности. Рекомендуется использовать архитектуру модели, представленной в публикации в журнале [Nature](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) со следующей модификацией: вместо одного выходного слоя нужно сделать два слоя, принимающих в качестве входа выход предшествующего скрытого слоя. **Обратите внимание**, данная модель отличается от модели, предложенной в домашней работе по DQN. Рекомендуется использовать ортогональную инициализацию с параметром $\\sqrt{2}$ для ядер свёрток и инициализировать смещения нулями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    общий принцип использования данной функции:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    для вычисления размерности входа полносвязного слоя\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "\n",
    "\n",
    "class Flatten(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.reshape(x.size(0), -1)\n",
    "\n",
    "\n",
    "class NatureModel(torch.nn.Module):\n",
    "    def __init__(self, bsize, nchannels, height, width, n_actions, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=nchannels, out_channels=32, kernel_size=8, stride=4\n",
    "        )\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(height, 8, 4), conv2d_size_out(width, 8, 4)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=4, stride=2\n",
    "        )\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 4, 2), conv2d_size_out(cur_w, 4, 2)\n",
    "        self.conv3 = torch.nn.Conv2d(\n",
    "            in_channels=64, out_channels=64, kernel_size=3, stride=1\n",
    "        )\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        cur_h, cur_w = conv2d_size_out(cur_h, 3, 1), conv2d_size_out(cur_w, 3, 1)\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = torch.nn.Linear(in_features=64 * cur_h * cur_w, out_features=512)\n",
    "        self.relu4 = torch.nn.ReLU()\n",
    "\n",
    "        self.fc_logits = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=512, out_features=64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(in_features=64, out_features=n_actions),\n",
    "        )\n",
    "        self.fc_values = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=512, out_features=64),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(in_features=64, out_features=1),\n",
    "        )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        torch.nn.init.orthogonal_(self.conv1.weight.data, gain=np.sqrt(2.0))\n",
    "        self.conv1.bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.conv2.weight.data, gain=np.sqrt(2.0))\n",
    "        self.conv2.bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.conv3.weight.data, gain=np.sqrt(2.0))\n",
    "        self.conv3.bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.fc1.weight.data, gain=np.sqrt(2.0))\n",
    "        self.fc1.bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.fc_logits[0].weight.data, gain=np.sqrt(2.0))\n",
    "        self.fc_logits[0].bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.fc_logits[2].weight.data, gain=np.sqrt(2.0))\n",
    "        self.fc_logits[2].bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.fc_values[0].weight.data, gain=np.sqrt(2.0))\n",
    "        self.fc_values[0].bias.data.fill_(0.0)\n",
    "\n",
    "        torch.nn.init.orthogonal_(self.fc_values[2].weight.data, gain=np.sqrt(2.0))\n",
    "        self.fc_values[2].bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.tensor(np.array(state), device=self.device, dtype=torch.float32)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "\n",
    "        logits = self.fc_logits(x)\n",
    "        value = self.fc_values(x)\n",
    "\n",
    "        x.cpu()\n",
    "        del x\n",
    "\n",
    "        return logits, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам также потребуется определить и использовать политику, которая будет использовать модель выше. В то время как модель вычисляет логиты для всех действий сразу и оценку функции ценности, политика будет сэмплировать действия, а также будет вычислять их логарифм правдоподобия. Метод `Policy.act` должен возвращать словарь всех массивов, требуемых для взаимодействия со средой и обучения модели. Обратите внимание, что действия должны быть формата `Numpy.ndarray`, в то время как другие тензоры должны быть в формате, определяемом библиотекой глубокого обучения (`Torch.tensor`, например)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        # <Реализуйте политику посредством прямого прохода модели, сэмплирования действий и вычисления их логарифмов правдоподобия>\n",
    "        # Должен возвращать dict с ключами ['actions', 'logits', 'log_probs', 'values'].\n",
    "        result = {}\n",
    "        logits, values = self.model(inputs)\n",
    "        result[\"logits\"] = logits\n",
    "        result[\"values\"] = values.flatten().contiguous()\n",
    "        dist = Categorical(logits=logits)\n",
    "        result[\"actions\"] = dist.sample()\n",
    "        result[\"log_probs\"] = dist.log_prob(result[\"actions\"])\n",
    "        result[\"actions\"] = result[\"actions\"].detach().cpu().numpy()\n",
    "        result[\"entropy\"] = dist.entropy()\n",
    "        return result\n",
    "    \n",
    "    def reset(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее требуется передать среду и политику в исполнитель `EnvRunner`, который собирает частичные траектории из среды. Класс `EnvRunner` уже реализован за Вас.\n",
    "\n",
    "Данный исполнитель взаимодействует со средой заданное количество шагов и возвращает словарь, содержащий ключи:\n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'resets'\n",
    "* 'actions'\n",
    "* и другие ключи, определённые в `Policy`\n",
    "\n",
    "по каждому из этих ключей содержится Python `list` соответствующих результатов взаимодействий со средой указанной длины $T$ &mdash; размера частичной траектории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы обучить часть модели, которая предсказывает ценности состояний, требуется вычислить целевые значения ценностей. В инстанцию класса `EnvRunner` можно подать при создании по аргументу `transforms` список вызываемых объектов (\"функций\"), которые последовательно будут применяться к частичным траекториям после сбора самих траекторий. Следовательно, требуется реализовать и использовать вызываемый (с определённым методом `__call__`) класс `ComputeValueTargets`. Формула для вычисления целевых значений ценности простая:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "Однако, не забудьте в реализации использовать `trajectory['resets']` флаги для проверки того, следует ли добавить целевые значения ценности на следующем шаге при вычислении целевых значений ценности на текущем шаге. У вас также имеется доступ к `trajectory['state']['latest_observation']` для получения последнего наблюдения в частичной траектории &mdash; $s_{t+T+1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, policy, gamma=0.99, device=torch.device(\"cuda:3\")):\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.policy = policy\n",
    "\n",
    "    def __call__(self, trajectory):\n",
    "        # Данный метод должен модифицировать траекторию trajectory на месте через добавление\n",
    "        # итеративно заполненного списка по ключу 'value_targets'.\n",
    "        # <Вычислите целевые значения ценности для данных частичных траекторий>\n",
    "        nsteps = len(trajectory[\"rewards\"])\n",
    "        trajectory[\"value_targets\"] = np.zeros((nsteps, nenvs), dtype=np.float32)\n",
    "        last_values = None\n",
    "        with torch.no_grad():\n",
    "            tmp_results = self.policy.act(trajectory[\"state\"][\"latest_observation\"])\n",
    "            last_values = tmp_results[\"values\"].detach().cpu().numpy()\n",
    "            del tmp_results\n",
    "        mask = 1.0 - trajectory[\"resets\"][-1]\n",
    "        trajectory[\"value_targets\"][-1] = mask * (\n",
    "            self.gamma * last_values + trajectory[\"rewards\"][-1]\n",
    "        )\n",
    "\n",
    "        for t in range(nsteps - 2, -1, -1):\n",
    "            mask = 1.0 - trajectory[\"resets\"][t]\n",
    "            trajectory[\"value_targets\"][t] = mask * (\n",
    "                trajectory[\"rewards\"][t]\n",
    "                + self.gamma * trajectory[\"value_targets\"][t + 1]\n",
    "            )\n",
    "\n",
    "        for key in trajectory:\n",
    "            if key == \"state\":\n",
    "                continue\n",
    "            if key in [\"logits\", \"values\", \"log_probs\", \"entropy\"]:\n",
    "                trajectory[key] = torch.stack(trajectory[key])\n",
    "            elif key == \"value_targets\":\n",
    "                # trajectory[key] = torch.tensor(np.stack(trajectory[key]), dtype=torch.float32, device=self.device)\n",
    "                trajectory[key] = torch.from_numpy(trajectory[key]).to(self.device)\n",
    "        return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После вычисления целевых значений ценности требуется преобразовать списки результатов взаимодействия со средой в тензоры с первой компонентой размерности `batch_size`, равной призведению `T * nenvs`, то есть требуется свести в одну компоненту первые две компоненты размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    \"\"\"\n",
    "    Сращивает первые две оси, обычно отвечающие за время и за инстанцию среды, соответственно.\n",
    "    \"\"\"\n",
    "    def __call__(self, trajectory):\n",
    "        # Модификация траектории на месте. \n",
    "        for key in trajectory:\n",
    "            if key not in [\"logits\", \"values\", \"log_probs\", \"entropy\", \"value_targets\"]:\n",
    "                continue\n",
    "            trajectory[key] = trajectory[key].flatten(0, 1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 617 # на своё усмотрение можно выбрать другой seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = NatureModel(*obs.shape, n_actions, device)\n",
    "model.to(device)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=5, # nsteps -- длина частичной траектории\n",
    "                                          # уменьшение nsteps может привести к вынужденному увеличению\n",
    "                                          # количества итераций оптимизации\n",
    "                   transforms=[ComputeValueTargets(policy=policy, device=device),\n",
    "                               MergeTimeBatch()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настало время реализовать сам алгоритм Advantage-Actor Critic (A2C). Его псевдокод можно посмотреть в [конспектах лекций (Раздел 5.2.5)](https://arxiv.org/pdf/2201.09746.pdf), в публикции [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) и в [лекции](https://www.youtube.com/watch?v=Tol_jw5hWnI&list=PLkFD6_40KJIxJMR-j5A1mkxK26gh_qg37&index=20) Сергея Левина."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes fraction of variance that ypred explains about y.\n",
    "    Returns 1 - Var[y-ypred] / Var[y]\n",
    "\n",
    "    interpretation:\n",
    "        ev=0  =>  might as well have predicted zero\n",
    "        ev=1  =>  perfect prediction\n",
    "        ev<0  =>  worse than just predicting zero\n",
    "\n",
    "    :param y_pred: the prediction\n",
    "    :param y_true: the expected value\n",
    "    :return: explained variance of ypred and y\n",
    "    \"\"\"\n",
    "    assert y_true.ndim == 1 and y_pred.ndim == 1\n",
    "    var_y = np.var(y_true)\n",
    "    return np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "class A2C:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy,\n",
    "        optimizer,\n",
    "        value_loss_coef=0.25,\n",
    "        entropy_coef=1e-2,\n",
    "        max_grad_norm=0.5,\n",
    "    ):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "    def policy_loss(self, trajectory):\n",
    "        # Тут нужно вычислить Advantages.\n",
    "        advantages = trajectory[\"value_targets\"] - trajectory[\"values\"].detach().clone()\n",
    "        trajectory[\"advantages\"] = advantages\n",
    "        trajectory[\"policy_loss\"] = -torch.mean(trajectory[\"log_probs\"] * advantages)\n",
    "\n",
    "    def value_loss(self, trajectory):\n",
    "        trajectory[\"value_loss\"] = F.mse_loss(\n",
    "            trajectory[\"values\"], trajectory[\"value_targets\"]\n",
    "        )\n",
    "\n",
    "    def loss(self, trajectory):\n",
    "        self.policy_loss(trajectory)\n",
    "        self.value_loss(trajectory)\n",
    "        trajectory[\"entropy_loss\"] = -torch.mean(trajectory[\"entropy\"])\n",
    "        trajectory[\"loss\"] = (\n",
    "            trajectory[\"policy_loss\"]\n",
    "            + self.entropy_coef * trajectory[\"entropy_loss\"]\n",
    "            + self.value_loss_coef * trajectory[\"value_loss\"]\n",
    "        )\n",
    "\n",
    "    def step(self, trajectory):\n",
    "        self.loss(trajectory)\n",
    "        self.optimizer.zero_grad()\n",
    "        trajectory[\"loss\"].backward()\n",
    "        total_norm = 0.0\n",
    "        for p in self.policy.model.parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            param_norm = p.grad.detach().data.norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "        trajectory[\"grad_norm\"] = total_norm ** 0.5\n",
    "        trajectory[\"explained_variance\"] = explained_variance(trajectory[\"values\"].detach().cpu().numpy(), trajectory[\"value_targets\"].detach().cpu().numpy())\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250000.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10_000_000 / 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно непосредственно обучить Вашу модель. С разумно подобранными гиперпараметрами обучение на одной GTX1080 на протяжении 10 миллионов шагов суммарно со всех батчированных сред (что переводится примерно в 5 часов работы) должно быть возможно достигнуть *среднюю исходную награду за 100 последних эпизодов* (значение переменной в `Tensorboard` по ключу `reward_mean_100`, усреднение берётся по 100 последним эпизодам в каждой среде в батче) **не меньше 600**. Это и будет считаться успешным результатом работы алгоритма `A2C`.\n",
    "\n",
    "**Внимание!** При *коректной* имплементации алгоритма *обоснованное* преодоление порога **400** по `reward_mean_100` *транслируется в* **8 баллов за задание**, *обоснованное* преодоление порога **600** по `reward_mean_100` на *корректно* написанном алгоритме обучения `A2C` *транслируется в* **10 баллов за задание**.\n",
    "\n",
    "Вам так же, по возможности, рекомендуется отобразить данную величину относительно `runner.step_var` &mdash; количества взаимодействий со всеми средами. Также очень рекомендуется предоставить графики следующих показателей (полезно для отладки кода):\n",
    "* [Коэффициент детерминации](https://en.wikipedia.org/wiki/Coefficient_of_determination) между целевыми значениями ценности и их предсказаниями\n",
    "* Энтропия политики $\\pi$\n",
    "* Функция потерь ценности (Value loss)\n",
    "* Функция потерь политики (Policy loss)\n",
    "* Целевые значения ценности (Value targets)\n",
    "* Предсказания значений ценности (Value predictions)\n",
    "* Норма градиента\n",
    "* Advantages\n",
    "* Общая функция потерь (A2C loss)\n",
    "\n",
    "В качестве оптимизатора рекомендуется взять метод [RMSProp](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) с [линейным убыванием шага](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html), начиная с 7e-4 до 0, константой сглаживания (alpha в PyTorch и decay в TensorFlow), равной 0.99 и epsilon, равным 1e-5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T01:43:12.609689Z",
     "start_time": "2024-04-16T01:43:12.605243Z"
    }
   },
   "source": [
    "Запуск Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_linear_schedule_with_warmup_lr_lambda(\n",
    "    current_step: int, *, num_warmup_steps: int, num_training_steps: int\n",
    "):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    return max(\n",
    "        0.0,\n",
    "        float(num_training_steps - current_step)\n",
    "        / float(max(1, num_training_steps - num_warmup_steps)),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, num_training_steps, last_epoch=-1\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after\n",
    "    a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.\n",
    "\n",
    "    Args:\n",
    "        optimizer ([`~torch.optim.Optimizer`]):\n",
    "            The optimizer for which to schedule the learning rate.\n",
    "        num_warmup_steps (`int`):\n",
    "            The number of steps for the warmup phase.\n",
    "        num_training_steps (`int`):\n",
    "            The total number of training steps.\n",
    "        last_epoch (`int`, *optional*, defaults to -1):\n",
    "            The index of the last epoch when resuming training.\n",
    "\n",
    "    Return:\n",
    "        `torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n",
    "    \"\"\"\n",
    "\n",
    "    lr_lambda = partial(\n",
    "        _get_linear_schedule_with_warmup_lr_lambda,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3110b1866614f28883a7cbe00d14b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_TRAIN_STEPS = 250_000\n",
    "optimizer = opt.RMSprop(model.parameters(), lr=7e-4, alpha=0.99, eps=1e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, 0, N_TRAIN_STEPS)\n",
    "a2c = A2C(policy=policy, optimizer=optimizer)\n",
    "\n",
    "for i in tqdm(range(N_TRAIN_STEPS)):\n",
    "    trajectory = runner.get_next()\n",
    "    a2c.step(trajectory)\n",
    "    runner.add_summary(\"Losses/loss\", trajectory[\"loss\"].item())\n",
    "    runner.add_summary(\"Losses/policy_loss\", trajectory[\"policy_loss\"].item())\n",
    "    runner.add_summary(\"Losses/value_loss\", trajectory[\"value_loss\"].item())\n",
    "    runner.add_summary(\"Values/predicted\", torch.mean(trajectory[\"values\"].detach().cpu()).item())\n",
    "    runner.add_summary(\"Values/targets\", torch.mean(trajectory[\"value_targets\"].detach().cpu()).item())\n",
    "    runner.add_summary(\"Values/entropy\", torch.mean(trajectory[\"entropy\"].detach().cpu()).item())\n",
    "    runner.add_summary(\"Values/advantages\", torch.mean(trajectory[\"advantages\"].detach().cpu()).item())\n",
    "    runner.add_summary(\"grad_norm\", trajectory[\"grad_norm\"])\n",
    "    runner.add_summary(\"Values/explained_variance\", trajectory[\"explained_variance\"])\n",
    "    runner.add_summary(\"lr\", scheduler.get_last_lr()[0])\n",
    "    scheduler.step()\n",
    "    if (i + 1) % 25000 == 0:\n",
    "        with open(f\"./checkpoints/step_{i}.pt\", \"wb+\") as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "with open(f\"./checkpoints/final.pt\", \"wb+\") as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "test_env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, clip_reward=False, summaries=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lives = 3\n",
    "\n",
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\"\n",
    "    Играем n_games игр до конца.\n",
    "    В случае жадной политики, выбираем действия как argmax(qvalues).\n",
    "    Возвращаем среднюю награду.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            output = agent.act([s])\n",
    "            action = output['logits'].argmax(dim=-1).item() if greedy else output['actions'][0]\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/imumtozee/workspace/MIPT_RL/task4/videos/False/rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/False\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=False) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/False/rl-video-episode-1.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'False').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/imumtozee/workspace/MIPT_RL/task4/videos/True/rl-video-episode-2.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# запись эпизодов\n",
    "with RecordVideo(\n",
    "    env=test_env,\n",
    "    video_folder=\"./videos/True\",\n",
    "    episode_trigger=lambda episode_number: True\n",
    ") as env_monitor:\n",
    "    sessions = [evaluate(env_monitor, policy, n_games=n_lives, greedy=True) for _ in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/True/rl-video-episode-1.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths = sorted([s for s in Path('videos', 'True').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[1]\n",
    "data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логи обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Средняя длина эпизодов\n",
    "\n",
    "<img src=\"./img/1.png\" width=640/>\n",
    "\n",
    "В данном графике можно увидеть, что по ходу обучения, эпизоды в среднем становятся длинее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Максимальный реворд со всех сред\n",
    "\n",
    "<img src=\"./img/2.png\" width=640/>\n",
    "\n",
    "Можно увидеть, что по ходу обучения максимальный реворд среди всех параллельных сред растет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Минимальный реворд со всех сред\n",
    "\n",
    "<img src=\"./img/3.png\" width=640/>\n",
    "\n",
    "Можно увидеть, что по ходу обучения минимальный реворд среди всех параллельных сред тоже растет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Средний реворд за последние 100 эпизодов\n",
    "\n",
    "<img src=\"./img/4.png\" width=640/>\n",
    "\n",
    "Основная метрика успеха в задании. Рост минимальных и максимальных ревордов со всех параллельных сред спустя некоторое время отражатеся на данном показателе и он положительно коррелирует с двумя графиками выше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Средний реворд за последний эпизод\n",
    "\n",
    "<img src=\"./img/5.png\" width=640/>\n",
    "\n",
    "Значение реворда за последний эпизод усредненное по всем параллельным средам. Можно увидеть что все графики относительно эпизода положительно коррелируют между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A2C Loss\n",
    "\n",
    "<img src=\"./img/6.png\" width=640/>\n",
    "\n",
    "На этом графике можно увидеть более менее стабильное течение обучения, то есть нет сильных осцилляций и модель следует exploration-exploitation tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Actor Loss\n",
    "\n",
    "<img src=\"./img/7.png\" width=640/>\n",
    "\n",
    "График показывает логарифм правдоподобия действий умноженных на advantage который они дают."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Critic Locc\n",
    "\n",
    "<img src=\"./img/8.png\" width=640/>\n",
    "\n",
    "График показывает среднюю квадратическую ошибку между value функцией предсказанной моделью и правильной (которая тоже все равно использует предсказания модели)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advantages\n",
    "\n",
    "<img src=\"./img/9.png\" width=640/>\n",
    "\n",
    "Разница между правильными значениями ценности состояний и теми которые предсказала модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Entropy Loss\n",
    "\n",
    "<img src=\"./img/10.png\" width=640/>\n",
    "\n",
    "Средняя энтропия действий. Добавляется к общему лоссу как регуляризатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Коэффициент детерминации\n",
    "\n",
    "<img src=\"./img/11.png\" width=640/>\n",
    "\n",
    "Доля дисперсии в распределении правильных значений ценности состояний объясняемой дисперсией предсказанных ценностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Предсказанные значения функции ценности\n",
    "\n",
    "<img src=\"./img/12.png\" width=640/>\n",
    "\n",
    "Наблюдается рост значений по ходу обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Целевые значения функции ценности\n",
    "\n",
    "<img src=\"./img/13.png\" width=640/>\n",
    "\n",
    "Так как при вычислении целевых значений функции ценности состояний используются предсказания модели, графики сильно похожи друг на друга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Норма градиентов\n",
    "\n",
    "<img src=\"./img/14.png\" width=640/>\n",
    "\n",
    "Норма градиентов весов до операции clip. Можно увидеть что по ходу обучения норма в среднем составляет то значение по которому и прозводится clip: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Информация об обучении"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прикрепление скриншотов графиков обучения модели в `Tensorboard` ниже является обязательным. Для доступа к `Tensorboard` запустите из командной строки в одной директории с данным ноутбуком следующую команду:\n",
    "```\n",
    "tensorboard --logdir logs --port 6006\n",
    "```\n",
    "В результате вывод в командную строку укажет, по какому адресу можно подсоединиться к инстанции `Tensorboard`, например, по адресу `http://localhost:6006/`. Оттуда можно и сделать скриншоты, демонстрирующие результаты обучения модели. Сами скриншоты с именем файла `image_name_x.png` для удобства лучше сохранить в директорию `./img`, откуда можно легко их прикреплять в `Markdown-клетках` ниже по команде со следующей конструкцией:\n",
    "```\n",
    "<img src=./img/image_name_x.png width=640>\n",
    "```\n",
    "Тут также требуется подписать изображения и дать небольшой комментарий по каждому скриншоту, что на нём описано.\n",
    "\n",
    "**Внимание!** В случае перезапуска процедуры обучения модели рекомендуется удалить директорию `./logs` вместе с её содержимым перед непосредственным перезапуском, чтобы не испортить отображающиеся графики в `Tensorboard`.\n",
    "\n",
    "**Совет.** При работе в Google Colab можно просто скачать директорию `./logs` и уже локально запустить `Tensorboard` для снятия скриншотов. Также можно обученного агента сохранить, скачать и локально на cpu запустить для записи роликов (для этого понадобится самостоятельно прописать код сохранения и загрузки модели в ноутбук из [файла](https://pytorch.org/tutorials/beginner/saving_loading_models.html)).\n",
    "\n",
    "**Внимание!** Посылку для сдачи задания требуется оформить в виде `.zip` архива, в котором будут *данный ноутбук*, использованные для его работы *скрипты*, *директории* `./videos`, `./logs` и `./img` с содержимым. Только так и не иначе!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Вставьте в данную ячейку свой ответ, подкреплённый скриншотами__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
